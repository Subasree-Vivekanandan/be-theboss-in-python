{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43938250-8f99-44ca-a177-bb6faf06d2ac",
   "metadata": {},
   "source": [
    "## Speech to Text using Python ðŸŽ¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b17108e-491d-43c1-a292-f5adcb5cb97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U \"assemblyai>=0.3.2\" -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15e51190-4930-423b-8611-fe41db562eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import ASSEMBLY_AI_TOKEN as api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "046e20e3-4854-4e28-9b70-0a9ea8efd4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have you been considering launching a product or even a business based on Python's AI ML Stack? We have a great guest on this episode, Dylan Fox, who is the founder of AssemblyAI and has been building his startup successfully over the past few years. He has interesting stories of hundreds of GPUs in the cloud, evolving ML models, and much more that I know you can enjoy hearing. This is talk. Python episode 356, recorded February 17, 2022. Welcome to talk python to me. A weekly podcast on Python. This is your host, Michael Kennedy. Follow me on Twitter, where I'm at M. Kennedy and keep up with the show and listen to past episodes at Talkpython FM and follow the show on Twitter via at talkpython. We've started streaming most of our episodes live on YouTube. Subscribe to our YouTube channel over at Talkpython FM YouTube to get notified about upcoming shows and be part of that episode. This episode is brought to you by the Stack Overflow Podcast. Join them to hear about programming stories and about how software is made and sentry. Find out about errors as soon as they happen. Transcripts for this and all of our episodes are brought to you by AssemblyAI. Do you need a great automatic speechtotext API? Get human level accuracy in just a few lines of code? Visit talkpython. FM AssemblyAI Dylan. Welcome to Talk Python to me. Yes, thank you. I am a fan, have listened to a lot of episodes and big podcast fan, so I'm happy to be on here. Thanks. Yeah, I'm sure you are. Your specialty is in the realm of turning voice into words. Yeah, that's right. I bet you do a lot of studying of media, like podcasts or videos and so on, right? Yeah, it's actually funny. So I started the company, I started assembly like four years ago, and there was this one audio file that I always used to test our speech recognition models on. It was this Al Gore Ted Talk from like 2007, I think. And I've almost memorized parts of that Ted Talk because I've just tested it so many times. It's actually still part of our end to end test suite. It's in there. It's like a legacy kind of founder thing that's like cool. Yeah, it is kind of funny, especially now that we're like 30 people at the company and I'll see some of the newer engineers writing tests around that Al Gore file still. And it makes me laugh because there's no real reason I picked that. It was just something easy that came to me. Yeah, you're getting started. I just got to grab some audio. Here's something. Right? Yeah, exactly. I've also listened to a ton of podcasts and we just started releasing models for different languages. And I was with someone from our team last week and I heard this phone call and this foreign language. People like, screaming on this. I was like, what are you listening to? And it is funny. As an audio company, you get sometimes data from customers and it's like you have to listen to it. Yeah, I bet there's some interesting stories in there. Yeah, for sure. Well, we're very privacy conscious. Not too many, but yeah, there was just on the verge. There was just an article about a different speech to text company. I don't know, have you seen this? That there was some suspicious stuff going on. Let me see if I can find it. I think what was it called? It was otter. AI. I'm not asking you to speak on them, but this a journalist otter AI scares a reminder that cloud transcription isn't completely private. Basically, there was a conversation about Uyghurs in China or something like that. Yeah. And then they unprompted reached out to the person who was in the conversation, said, could you tell us the nature of why you had to speak about this? No way. They're like what? That is crazy. We're a little concerned about why. It's kind of like, interested in the concept of our conversation. Yeah, there's a lot of that suspicion around. There's some conspiracy gears right around, like, oh, does your phone listen to you while you're around? And does it ambiently listen to you and then use that data to remarket to you? And I was talking to someone about this recently. I did nothing about the location based advertising world, but sometimes I'll be talking about something and then I'll see ads for it on my phone or if I'm on Instagram or something. And someone told me it's probably more based on the location that you're in and the other data that they have about you. Yeah. You were at your friend's house. Your friend just searched for that, then told you about it. Yeah, exactly. I think the reality of that is that it's actually more terrifying than if they were just listening to you. They can piece together this shadow reality of you that matches reality so well. Yeah. Like, your friend just bought this thing and you went over and then so maybe you're interested in this thing because you're probably they probably told you about it or something, right? Yeah. It is really crazy. It is really crazy. I haven't paid too much attention to all the changes that are happening around. Like, I listened to some podcast, I think, on the Wall Street Journal about the big change that Google is making around that tracking and how a lot of people are up in arms about that. And it was saying something, how they're going to have and sorry if I'm derailing whatever plan we had for a conversation here. You're derailing in a way that I'm super passionate about because it's so crazy. But yeah, don't go too deep. But yeah, it's still interesting. They said that there's probably butchering this, but something how for each user, they're just going to have six categories about you and then one of them is going to be randomly inserted as to somehow anonymize your profile. And I just thought, yeah, super weird to hear about how they're doing and what the meeting was internally that came up with that idea. So I'm like, well, let's just throw a random category on there. I don't know, my thoughts are we're faced or were presented with a false dichotomy. Either you can have horrible, creepy tracking advertising, shadow things like we talked about, or you can let the creators and the sites that make the services you love die. There are more choices than two in this world, right? For example, you could have some kind of ad that is related to what is on the page rather than who is coming to the page. You don't necessarily have to retarget me. For example, right here on this podcast, I'm going to tell people about I'm not sure, without looking at the schedule, what the advertisers are for this episode, but I am going to tell people about that and it is exactly related to the content of this show. It's not that I found out that Sarah from Illinois did this Google search and visit this page. So now we're going to show no, there's so many sites like this one here on The Verge. You could have ads for AssemblyAI and it would be maybe you don't actually want on this one, but things like this, it would be totally reasonable to put an ad for speech to text companies on there that requires no targeting and no evil shadow companies. And we can go on and on. But there are many ways like that, right, that don't require this false dichotomy that is being presented to us. So hopefully we don't end up with either of those because I don't think those are the best options or the only options. Yeah, it's weird how that's kind of how things developed to where we are now. Yeah, but I agree with you. There's probably a lot we could unwrap everyone's looking for, like, okay, well, if we do retargeted, we can get 2% better returns and no one's worried about, well, what happens to society. Yeah, that's actually what I was going to say. It's all about the kind of high growth, like society that we have where we need to maximize growth and maximize returns. And I mean, I understand this acutely. Like, I'm the CEO of a startup, so I get it. But yeah, it's when it's like growth over everything, you end up with things like what you said, like, oh, it increases our returns 2%, so let's do this. But you don't think about what the trade offs will be? Yeah, absolutely. All right, well, thanks for that diversion. That was great. Yeah. But before we get beyond it, let me just get your background real quick. How do you get into programming? And I'm going to mix it up a little and machine learning definitely. Do you want the long story or the short story? How many minutes do I intermediate. Intermediate. Intermediate. So the intermediate story is that I started a company when I was in college, just like a college startup thing, and at the time was very limited in my programming knowledge. I had done some basic HTML when I was a kid. I was really into counterstrike and Call of Duty. I would sell private servers. I don't know how I got into this, but I rented these servers and I would like, window remote, Windows desktop into them and set up private Counterstrike servers and then sell those and set up a basic website for it with HTML and CSS. And my brother was super into computers, so it was always kind of into computers and then in college got into startups. And I think programming and startups are really connected. So through that, learned how to code, learned how to program, started attending Python Meetups in Washington DC, where I went to school. And that's how I met Matt Mackay, who's a mutual mutual connection. Yeah. So attended a bunch of meetups, learned how to program, and then got really into it. But I think what I found myself more interested in was the more like meaty programming problems. More like, I guess, like algorithm type problems. And that kind of naturally led me to machine learning and NLP and then kind of just took off from there because I found that I was really interested in machine learning and different NLP problems. Those are obviously the really hard ones, especially probably when was this that you were doing it? This is maybe like 2013, 2014. Kind of the early days of when that was becoming real. Right. I remember feeling like all this AI and text to speech or speech to text rather type of stuff was very much like fusion. Like 30 years out, always 30. It's going to come eventually, but people are doing weird stuff in Lisp and it doesn't seem to be doing much of anything at all. Perl script seat out. Yeah. And then all of a sudden, we end up with amazing speech attacks. We end up with self driving cars. Like something clicked and it all came to life. Yeah, it's kind of crazy, especially over the last couple of years. I think what's really interesting is that a lot of the advances in self driving cars and NLP and speech and text, they're all based on similar machine learning algorithms. So, like, the Transformer, right, which is a really popular type of neural network that came out, was initially applied towards just NLP, like text language modeling related tasks. Now that's shown to be super powerful for speech as well. Whereas classical machine learning, there were still these underlying algorithms like support vector machines or other types of underlying algorithms, but a lot of the workers around the data and so how can you extract better features for this type of data? And you had to be I remember when I was getting into speech recognition, I bought this speech recognition, like, textbook, and this is a while ago, and it was around really understanding phonemes and how different things are spoken and how the human speech is spoken. And now you don't need to know about that. You just get a bunch of audio data and you train these, like, big neural networks. They figure that out. Right. You wanted to understand British accents and American accents. You just give it a bunch given more data mix, right? Yeah, exactly. But it is crazy to see where things have gotten over the last couple of years in particular. Yeah. So when I was starting out, neural networks were there, but they're a lot more basic, and you didn't have, like there's a lot more compute resources now, more mature libraries like TensorFlow and PyTorch. I think I went to one of the first TensorFlow meetups that they had or not meetups, like, developer days or whatever down at the Google conference. So it's like so new still. Yeah, it's so new. Yeah, it's easy to forget. It is. A while ago it is that all this stuff didn't even exist. Right? Yeah, absolutely. So you mentioned AssemblyAI. Yes. That's what you're doing these days, right? Yeah. So I am the founder of a company called AssemblyAI. We create APIs that can automatically transcribe and understand audio data. So we have APIs for automatic speech to text of audio files, live audio streams, and then APIs that can also summarize audio content, do content moderation on top of audio content, detect topics, what we call like, audio intelligence APIs. And so we have a lot of startups and enterprises using our APIs to build the way we call it, applications on top of audio data, whether it's like content moderation of a social platform or speeding up workflows. Like, I'm sure you have where you take a podcast recording and transcribe it so you can make it more shareable or extract pieces of it to make it more shareable. Yeah, exactly. Yeah. Basically for me, it's a CLI command that runs Python against your API, against a remote MP3 file and then magic. That's the great thing about podcast hosts that are also programmers. I've talked to a few, and they're all like, there's a bunch that are non programmers and they use these different services. But every podcast host that I've talked to that's a programmer, they have their own CLIs and Python scripts that they're running. Yeah, there's a whole series of just clising commands to do the workflow. Yeah. I do want to just give a quick statement disclaimer. Yes. So if you go over to the transcripts or possibly, I suspect if you listen to the beginning of this episode, it'll say that it's sponsored by AssemblyAI. This episode is not part of that sponsorship. This is just you and I got to know each other you're Doing Interesting Stuff. You've been on some other shows that I've heard that the conversation was interesting. So invited you on. Thank you for sponsoring the show. But just to point out, this is not actually part of that. But the transcripts that we do have on the show the last year or so are basically generated from you guys, which is pretty cool. Yeah. And we don't even need to talk about assembly that much on this podcast. We can talk about other things. Yeah. So one of the things I want to talk about and maybe what's on the screen here gives a little bit of a hint of being TensorFlow is why do you think Python is popular for machine learning startups in general? I feel that I'm not as deep in that space as you. But looking in from the outside, I guess I would say it feels very much like Python is the primary way at which a lot of this machine learning stuff is done. Yeah, it's a good point. So why that is outside of machine learning even I think Python is just such a popular language because it's so easy to build with compared to like PHP or C sharp and even JavaScript. When I learned to code, I started with Python because the syntax was easy to understand. There were a lot of good resources. And then there's kind of this snowball effect where more people know Pythons, there's more tutorials about Python, there's more libraries about Python, and it's just more popular of a language. Yeah. This insights you pulling this up. Yeah. But if you pull up the stack overflow trends for the most popular programming languages, there's only one that is going dramatically up out of ten languages or something. It's just so much more popular. Yeah, it is. It's so interesting how it's really sort of taken off and it wasn't back in when you got started and when I got started back in this general area, interesting. Twelve was the number one language then. The number one then was what is that? C Sharp. C Sharp. But you got to keep in mind this is a little bit of a historical bias of stack overflow. Stack overflow was started by Jeff Atwood and Joel Spolski, who came out of the net space. So when they created like its initial traction was in C sharp and VB. But over time, clearly, it's become like where programmers go. Obviously. So take that a bit with a grain of assault. But that was the number one back in the early days. Another Founder Legacy decision. Yes, exactly. I agree that it's absolutely generally popular, and I think there's some interesting reasons for that. Yeah. It's just so approachable. But it's not a toy. Right. A lot of approachable languages are Toy languages, and a lot of non Toy languages are hard to approach. This portion of talk Python to me is brought to you by the Stack Overflow podcast. There are few places more significant to software developers than Stack Overflow, but did you know they have a Podcast? For a dozen years, the Stack Overflow Podcast has been exploring what it means to be a developer and how the art and practice of software programming is changing our world. Are you wondering which skills you need to break into the world to technology or level up as a developer? Curious how the tools and frameworks you use every day were created? The Stack Overflow Podcast is your resource for tough coding questions and your home for candid conversations with guests from leading tech companies about the art and practice of programming. From Rails to React, from Java to Python, the Stack Overflow Podcast will help you understand how technology is made and where it's headed. Hosted by Ben Popper, Cassidy Williams, Matt Kiernander and Sierra Ford. The Stack Overflow podcast is your home for all things code. You'll find new episodes twice a week. Wherever you get your Podcast, just visit Talkpython FM StackOverflow and click your Podcast player icon to subscribe. One more thing. I know you're a Podcast veteran and you could just open up your favorite Podcast app and search for the Stack Overflow Podcast and subscribe there. But our sponsors continue to support us when they see results, and they'll only know you're interested from hawk Python if you use our link. So if you plan on listening, do use our Link Talkpython FM StackOverflow to get started. Thank you to Stack Overflow for sponsoring the show. Yeah, for me, it was very easy to get started with Python, and I actually had, so I taught myself how to program. I went to college, I studied economics, so did not study college programming in college, computer science. And the first language I started to try to learn was PHP, and I bought this huge PHP textbook and made it halfway through, and I was like, what is going on? I gave up and then tried again with Python later and it was so much easier. And then I also wonder how much of this is for the machine learning libraries. In specific, you have these macro trends where a lot of the data science boot camps that have been so popular, there's like ScikitLearn, I know we have a tab up there. There's like NumPy, and NLTK is one of the popular NLP libraries. So there are a lot of libraries in Python. In the early, when I was getting into NLP, I worked a lot with NLTK and Sci, Pi and ScikitLearn and NumPy, and I think a lot of work was done around there. And so people that were doing data science or doing some type of machine learning were already in Python. And then now you have like, PyTorch and TensorFlow and it's just like kind of cemented, like okay, the machine learning libraries today, the popular ones, you work with them in Python. Yeah. You want to give us your thoughts on those. We've got TensorFlow and PyTorch and probably ScikitLearn as well. Those are the traditional ones. You've got newer ones, like hugging face. Yeah, they're a cool company. Maybe give us a survey of how you see the different libraries ML space and the libraries that people might choose from. So when we started the company, everything was in TensorFlow. When was that? Back in late 2017. Okay. Yeah, late 2017, everything was in TensorFlow. And actually, I don't know what year PyTorch came out. I want I don't even know if it was out back then, or maybe it was, like, just internally. Facebook. Yeah. It's pretty new. Yeah. So TensorFlow was definitely they got started early. I think their docs and the framework just got complicated over the years. And then they sort of rebooted with, like, TensorFlow 2.0, and then there was Keras. That was popular. It kind of got pulled in now, I think. So we switched everything over to PyTorch in the last year or two. A big reason for that was that and we actually put out this article on our blog comparing PyTorch and TensorFlow. And we have this chart where we show the percentage of papers that are released, where the code for the paper is in PyTorch versus TensorFlow. And it's a huge difference. Like, most of the latest research gets implemented. Yeah. Here it is. If you go down to one of so this is Hugging fees. Can you go keep going? Yeah. Research papers. Yeah, go up to that one. Yeah. Okay. So it shows, like, the fraction of papers. And so what we're showing here for the people that are listening is like a graph that shows the percentage of papers that are used built using PyTorch versus TensorFlow over time. Yeah. When you started, it was what is this? Six, 7%? Probably Tensorch. And the balance being TensorFlow, you started your company, and now it's 75% PyTorch. That's a huge, very large, dramatic change. If PyTorch was a company, it'd be, like, probably raising a lot of money. I think one of the reasons we picked PyTorch is because a lot of the newer research was being implemented in PyTorch. First, there were examples in PyTorch, and so it was easier to get they have it on their tagline, but to quote them from research to production right. It was easier to get more exotic advanced neural networks into production and actually start training models with those different types of layers or operations or loss functions that were released in these different papers. So we started using PyTorch, and we kind of haven't looked back. Well, if you're tracking all the research and trying to build a cutting edge startup around ML, you don't want to wait for this to make its way to other frameworks. You want to just grab it and go right. That's where the research is being done. That helps a lot. Right, right. Exactly. Yeah. You can just get up and running a lot faster with the newer research. And so most companies that I talk to now, they're all using PyTorch. I think PyTorch is definitely like the more popular framework. There's some new ones coming out that have people excited. But still, from what I can sense, PyTorch is if someone was going to get started today, I would tell them to start with PyTorch. Yeah. And I think TensorFlow is also PyTorch. I think who runs PyTorch is released by Facebook, right? Yeah. And then TensorFlow. That's Google, right? Yeah. And I think Google's tried to tie TensorFlow into their cloud ML products or train your models on Google cloud and use their TPUs in the cloud. And there's probably some business cases behind that, but I feel like it may have made the developer experience worse because it's trying to get back to Google, whereas PyTorch isn't trying to get you to train your models on Facebook cloud or something. Yeah. What's the story with hugging Face? People probably wouldn't use Facebook cloud if that existed nowadays. I don't know if you'd want to host your data metacloud. Metacloud now? Yeah, metacloud. You can only do it in VR. Yeah. What's the story with hugging Face? So, Hugging Face is a cool so this is a company, actually, and they have it's kind of hard to even explain. It's like you could basically get access to a bunch of different pre trained models really quickly through hugging Face. And so if you want to a lot of work around NLP now is how familiar with self supervised learning or base models for NLP? How familiar with that? Somewhat. So the idea is to have a general model and then apply some sort of transfer learning to build up a more specialized one without training from scratch, is that exactly? Yeah. And that general model is really just trained to learn representations of the data. It's not even really trained with our particular NLP task. It's just like, trained to learn representations of data, and then with those representations that it learns, you can then say, like, okay, I'm going to train you towards this specific task with some labeled data in a supervised manner. And so there are some really popular open source base models, foundation models. Like Burt is one, there's a bunch of others, but you can easily get load up Burt, basically, and fine tune it on your data with hugging Face. So if you're trying to get a model up and running quickly in the NLP, like the text domain, you can do that pretty easily with hugging Face. Okay. Yeah. So interesting, it's less like if you want to build your own neural network from scratch, like inputs to outputs, implement your own mouse function, all that. You do that in PyTorch. If you want to try to just quickly fine tune Burt for a specific task that you're trying to solve, you could still go, like, the PyTorch route, but it would just be faster to go tugging base. So they've seen a lot of adoption there and then ScikitLearn is kind of like the old school library that's been around forever with the OG. If you want to do stuff with support vector machines or random forest or like KDR's neighbors, ScikitLearn is probably still really popular in that for those different use cases. I do think I hear ScikitLearn being used quite a bit still. Yeah, maybe in the research, the academic, if you go take a course on it, probably there's a lot of stuff on this, I would guess. Yeah, there's a lot of times where, I mean, you don't really need to build a neural network. I mean, there's parts of our stack that are like basic machine learning, like statistical models, and if you can get away with it, it's a lot easier to train and you don't need as much data and it's easier to deploy. So a lot of recommendation type models and sometimes SBMs are just like good enough. SVM support vector machines are just good enough for a task that you might want to have for a lightweight Netflix recommendation or YouTube recommendation. Not like the high end stuff that I'm sure they're actually doing. Yeah, something like that. Yeah, exactly. That kind of recommendation engine. Yeah, something basic. Yeah. Although I actually am kind of underwhelmed with the Netflix and YouTube recommendations are very good. Netflix recommendations and prime recommendations. I'm kind of underwhelmed by you would think that you watch. I agree. Yeah. It's still so hard to find things to watch sometimes on those platforms. It is. And YouTube interestingly, seems to have an end. So if you scroll down through YouTube, like ten pages, it'll start showing you like, well, it seems like we're out of options here. We'll show you ten from this one channel and then we'll just kind of stop. I know you got a lot of videos, you could just keep recommending stuff. I'm pretty sure if you would keep recommended there's stuff down here, but yeah, I agree. It's interesting. I feel like it's gotten better too. Like my YouTube consumption has really picked up over the last year, I would say the recommendation algorithms. And I don't know if it's just more content being created or maybe it's just like a personal thing for me. And there was some thing on Hacker News too about YouTube. Comments that one of the founders of Stripe posted are generally very positive. There's really good comments on YouTube too, so they've definitely also come up with ways to classify comments as being high value or not, and then put those up top. And nowadays those models are definitely used with something like some big neural networks, some transformer, because those neural networks, they're so much better at understanding context. And like SBMs, you have to still for a lot of these classical machine learning approaches like feed it hand labeled data. But the neural networks, they're really good for those language tasks now. Yeah, absolutely. Christopher out in the audience has a question that's kind of interesting. Does it make sense to start with ScikitLearn if, for example, you're trying to predict when a production machine is not out of tolerance yet is trending to be, is that like if you were like monitoring like a data center for maybe VMs? Like you're guessing Ram or like memory is going high or some statistic is like predictive that this VM will probably go down. Failure is coming. Failure is coming. Yeah. And the question was, is SVM or ScikitLearn good to start with? Yeah, I would actually probably say that's where you want to go with something like ScikitLearn because there's probably very clear cut patterns. I would say if you're unsure of what the pattern is, then a neural network is good because the neural network can in theory, like you're feeding it raw data and it's learning pattern. But if you know what the pattern is like, okay, there's probably like these signals that if a human was just sitting there looking at it all day would be able to tell this system is probably going to go down. Then you just can train an SVM or some type of classical machine learning model with ScikitLearn to be able to do those predictions with pretty high accuracy. And then you've got a super lightweight model. You don't need much training data to train it because you're not trying to build something that's super generalizable to all systems or all AWS instances. It's probably something unique to your system. But I would say that's kind of where the difference is. And then it's a lot easier too, because if you're trying to build like a neural net, it's like, well, what type, how many layers, what kind of optimization, schedule, learning rate. There's all these hyperparameters and things you have to figure out. You still have to do that too for classical machine learning to a degree. But if your problem is not that difficult, it's not as fancy nowadays, but it gets the job done. Yeah, I suspect you could come up with some predictors and then monitor them in this model. Whereas opposed to here's an image that is a breast scan. Does it have cancer or not? Right, exactly. We don't even really know what we're looking for. But there probably is a pattern that could be pulled out by a neural network. Exactly. Yeah, that's a great point. And we're trying to build some predictive scaling for our API right now because one of the problems with the challenges of a startup that's doing machine learning in production is we deploy like hundreds of GPUs and thousands of CPU cores into production every day at peak load and you have to be able to scale to demand. And if you overscale at that size, then there's just huge costs that come with that. If you underscale there's bad performance for end users. And so we've done a ton of work around auto scaling and trying to optimize models in production and things like that. And now we're trying to do some predictive scaling. And for that, for example, we'd probably do something super simple with like, ScikitLearn. We want to do a neural net for that. Yeah, the scaling sounds like solving basically a similar issue yeah. As understanding failure, right? Yeah, exactly. The lack of scaling sometimes is kind of the result is failure. They're somewhat related together. Yeah. You talked about running stuff in production and there's obviously two aspects for machine learning companies and startups and teams and products that are very different than, say, the kind of stuff I do. Right. Like, I've got APIs that are running, we've got mobile apps, we've got people taking the courses. But all of that stuff there is like one, it's always the same. We put stuff up and people will use it and consume it and so on. But for you all, you've got the training and almost the R and D side of things that you've got to worry about working on and scaling and you've got the productionizing. So maybe tell us a little bit about what do you guys use for both parts? Training, maybe start with the training side. Yeah, the training side. It's basically like impossible to use the big clouds for that because it would just be prohibitively expensive, at least for what we do. So we train these huge neural nets for speech recognition and different NLP tasks and we're training them across like 40, 64 GPUs, like really powerful GPUs. I've got the GeForce 30 90, which is beast up here. Do you know what kind you're using? Yeah, so we use a lot of V 100, like a 100. And basically what we do is we rent dedicated machines from provider, and each machine we're able to pick the specs that we want, like how many GPUs, what cards, how much Ram, what kind of CPU we want on there. So we're able to pick the specs that we want. And we found that that's been the best way to do it. Because the big clouds, if you're running dozens of GPU, of the most expensive types of GPUs for like weeks on Ed, you could do that if you had like one training run you wanted to do. But a lot of times you have to train a model halfway through. It doesn't work well, you have to restart or finish this training and the results are not that good. And you learn something, you have to go back and start over. And now what we're doing is buying a bunch of our own Compute. My dream is to have some closet somewhere with just like tons of GPUs and our own mini data center for the R and D. Because if things go down when you're training a model, you checkpoint it as you go. So if your program crashes or your server crashes, you could resume training. Whereas, like for production workloads, we use AWS for that because things can't go down. And I don't think we'd want to take on our incompetency of hosting our own production infrastructure. But for the R and D stuff, we are looking into just buying a ton versus renting because it'd be a lot more cost efficient. And instead of basically paying each year for the same compute, you just buy it once and then you just pay for the electricity and server hosting costs and maintenance costs that come with that. Maybe find a big office building and offer to heat it for free in the winter by just running it on the inside. There's this like you can run like, Nvidia SMI. I don't play around with GPUs at all, but you can see what the temperature is of the GPU. And sometimes I remember a while ago when I was training some of these models, I would just look at what the temperature is during training and yeah, they get so hot. And these data centers have to have all these special cooling infrastructure to keep the machines down. It's pretty environmentally unfriendly. Yeah. To the extent that some of them to the extent that people are creating underwater data center like nodes and putting them down there and just letting the ocean be the heat sink. Yeah. That's crazy. You can buy some land Antarctica, and put our stuff there. That's where the GitHub the Arctic Code thing, I forget what it's called. Yeah. The Arctic Code vault. Yeah. So we could do something like that for our GPUs when we get bigger. That's the dream. That's where I might nerd out. There you go. I think we have, I think, somewhere like maybe like 200 GPUs that we use just for R and D and training, and we're getting a lot more because you don't want to be a lot of times there's like, scheduling bottlenecks. So two researchers want to run a model and need a bunch of compute to be able to do that, or they're both good ideas. You don't want to have to wait four weeks for someone to run their model because compute is taken. So we're trying to unblock those scheduling conflicts by just getting more compute. And for the production side, yeah, we deploy everything in AWS right now and onto smaller GPUs, because a lot of our models do inference on GPU still. Some of our models do inference on CPU. Interesting. To evaluate the stuff, it still uses GPUs. Correct models are created correct. Yeah, we could run it on CPU, but it's just not as parallelizable as running it on GPUs. There's a lot of work that we could probably do to get it really efficient so that we're running it on as few CPU cores as possible. But one of the problems is almost like every three to four months, we're throwing out the current neural network architecture and using a different one that is giving us better results. Sometimes we'll make the model bigger or there'll be a small Tweak in the model architecture that yields better results. But a lot of times it's like, okay, we've kind of iterated within this architecture as much as we can. And now to get the next accuracy bump, we have to go to a new architecture. We're undergoing that right now. We released one of our newer speech recognition models. We released, I think, like three months ago and the results are really good. But now we have one that is looking a lot better and it'd be like a completely different architecture. And so it's just that trade off of do you spend a bunch of time optimizing the current model that you have and trying to prune the neural network and do all these optimizations to get it really small? Or do you just spend that research effort and that energy focused on finding the next accuracy gain and because we're trying to win customers and grow our revenue, it's just, all right, let's just focus on the next model. And when we have a big enough team or when we can focus on it, we'll work on making the models smaller and more compute efficient and less costly to run. But right now, yeah, our speech recognition model that does inference on a GPU, there's a couple of our NLP related models, like our content moderation model that does inference on a GPU. And then there's our automatic punctuation and casing restoration model like, that runs on a CPU because that's not as compute intense. So it really varies. Yeah, it's pretty interesting to think about how you're optimizing the software stack and the algorithms and the libraries and whatnot. When you're not doing something that's changing so quickly, if it's working, you can kind of just leave it alone. Right? I've got some APIs. I think they are built either in pyramid or flask. Sure, it'd be nicer to rebuild them in fast API, but they're working fine, I have no reason to touch them. Right, so there's not like a huge step jump I'm going to take. They're not under extreme load or anything. Right. This portion of Talkpythonomy is brought to you by Sentry. How would you like to remove a little stress from your life? Do you worry that users may be encountering errors, slowdowns or crashes with your app right now? Would you even know it until they sent you that support email? How much better would it be to have the error or performance details immediately sent to you, including the call stack and values of local variables and the active user recorded in the report? With Sentry, this is not only possible, it's simple. In fact, we use Sentry on all the talkpython web properties. We've actually fixed a bug triggered by a user and had the upgrade ready to roll out as we got the support email. That was a great email to write back. Hey, we already saw your error and have already rolled out the fix. Imagine their surprise, surprise and delight your users. Create your Sentry account at Talkpython Fmcentry and if you sign up with the code Talkpython all one word, it's good for two free months of Sentry's business plan, which will give you up to 20 times as many monthly events as well as other features. Create better software, delight your users, and support the podcast. Visit Talkpython Fmcentry and use the coupon code Talkpython. But in your world, there's so much innovation happening around the models that you do have to think about that. So how do you work that trade off? How do you like, well, could we get more out of what we've got or should we abandon it and start over? Right, because it is nice to have a very polished and well known thing as well. Definitely. And every time you throw out our architecture and implement a new architecture, you've now got to figure out how to run that architecture at scale. And you don't want to have any hiccups for your current customers or users of your API, which sometimes happens because these models are so big that you can't just write this model service that sits on a GPU and does everything. You have to break it up into a bunch of component parts to let it so that you can run it efficiently at scale. So there's like eight, nine microservices for a single model because you break out like, okay, all these different parts and try to get it running really efficiently in parallel. But it does beg the question of how do you build good CI CD workflows and good DevOps workflows to get models into production quickly? And this is something that we're working on right now and trying to solve. A lot of times we have better models and we sit on them for like two, three weeks because to get them into staging, we have to do load testing. See, does anything with scaling have to change because the model profile is different? Are there any weird edge cases that we didn't check or see during testing? So it slows down the rate of development because it's hard to do CI CD. It's not like you just, okay, run these tests, the code works, go there's like compute profile changes that happen. And so maybe you need a different instance type, or you need to use less CPU, but way more Ram. So if you actually going to crash or something okay, exactly. And then doing that at scale, you have to profile that and do load testing. And so really, we're trying to figure out how to get these models into production faster. And I think the whole ML Ops world is so in its infancy around things like that. And it's a lot of work. Yeah, it's a lot of work. So for us, the trade off, though, is always like, our customers and developers, they just want better results and always more accurate results. And so we just always are working on pushing our models, making them more accurate if we can. Iterate within a current architecture, great. Sometimes you can just make the model bigger or make a small change, and then you get a lot of accuracy improvements and it's just like what we call like, a drop in update where no code changes. It's just literally like the model that you're loading is just different and then it's just more accurate. Right, that's easy. That's the dream. It's just a drop in. But that's maybe like, 30% of updates. Like, the other 70% are okay, you've got a new architecture or it's got a pretty different compute profile, so uses a lot more Ram or it's a lot slower to load in the beginning. So we need to scale earlier because instances come online later and become healthy later. So there's all these things you have to think about. Yeah. The whole DevOps side of this sounds way more interesting and involved than I. Yeah, it's painful too. I can't explain how many graphs we have in datadog, just like, monitoring things all day. Luckily, I don't have to work on that anymore. That was very stressful when I was, like, owning the infrastructure. Now we have people that are better at it than me. We had like, two DevOps people start on Monday, but yeah, DevOps is a huge piece of this. Yeah, that's quite interesting. Yeah. I do want to just circle back to R1. Quick thing, you talked about buying your own GPUs for training, and people might out there be thinking, like, who would want to go and get their own hardware in the day of AWS node, whatever. Right. It just seems crazy, but there's certainly circumstances like, here's an example that I recently thought about. So there's a place called Mac Stadium where you can get Macs in the cloud. How cool, right? So maybe you want to have something you could do with extra things and well, what does it cost? Well, for a mac mini M One, it's $132 a month. You think that's is that high or low? Well, the whole device, if you were to buy it, costs $700. Yeah. And I suspect that even though the GPUs are expensive, there's probably something where if you really utilize it extensively, it actually makes to buy it. It stops making sense in ways that people might not expect. Yeah. It's a buy it, you mean. Right, like, it stops making sense to rent? Yeah, that's what we're stopped making sense to rent it in the cloud. Yeah, I mean, we spent a crazy amount of money renting GPUs in the cloud and it's like, okay, if we had a bunch of money to make a capex purchase, right? Like, just shell out a bunch of money to buy a bunch of hardware up front, it'd be so much better in the long run because it is similar to the example you made about if you don't have a lot of cash, then you're only going to use a Mac for a couple of months. Right. You need it for two weeks, then it doesn't make sense to buy it. Great, you pay the $100 and you're good. Right. Or if you don't have two K, then you just rent it. And if you don't have the money to buy a house, you rent an apartment. Right. Like things like that. So there are definitely benefits. And I think for most models, you don't need crazy compute you could get away with. You could buy a desktop device that has two GPUs or you could rent a dedicated machine or still do it on AWS if you're using like one or two GPUs and it would be insane. So if you're just starting out, all those options are fine. But if you're trying to do like big models or train a bunch in parallel, you need more compute and definitely doesn't make sense to use the big clouds for that. There's a bunch of dedicated providers that you can rent, like dedicated machines from just pay a monthly fee regardless of how much you use it and it's a lot more efficient for companies to do that. Interesting. Give me your thoughts on sort of Capex versus Opex for ML startups rather than, I don't know, some other SaaS service that doesn't have such computational stuff. Capex being you got to buy a whole bunch of machines and GPUs and stuff versus Opex, well, it's going to cost you in the cloud. I feel like it's crazy. Things are more possible because you can get the stuff in the cloud, prove an idea and then get investors without going, well, let's go to friends and family and get 250,000 for GPUs. And if it doesn't work, we'll just do bitcoin. Yeah, definitely. I mean, we started in the cloud, right? So like, first models we trained were K 80s on K. AWS took like a month to train. Wow. Yeah, it was terrible. So we started in the cloud and then now that we're fortunate to have more investment in a company, we can make these Capex purchases. But yeah, I mean, the operating expenses of running a ML startup are also like crazy. Like payroll and payroll and like AWS are biggest expenses because you run so much Compute and it's super expensive. And what I talk about and what we talk about is there's nothing fundamental about what we're doing that makes that the case. It just goes back to that point of like, do you spend a couple of months optimizing your models, bringing Compute costs down or do you just focus on the new architecture and kind of pay your way to get to the future? Like this growth versus yeah, and then we're like a venture backed company, so there's expectations around our growth and all that. So we just focus on like, okay, let's just get to the next milestone and not focus too much on bringing those costs down. Because there's the opportunity cost of doing that. But eventually we'll have to. Yeah, it's a little bit of the ML equivalent of sort of the growth. You can lose money to just gas users, but this is the sort of gain it is capabilities, right? Yeah, it is, 100%. And then you'll figure out how to do it efficiently once you find your way. Okay. And I'll give you a tangible example. I mean, we've been adding a lot of customers and developers on the API, and there's always, like, new scaling problems that come up. And sometimes we're just like, look, let's just scale the whole system up. It's got to be inefficient, there's got to be waste, but let's scale it up. And then we'll fine tune the auto scaling to bring it down over time versus having to step into a more perfect auto scaling scenario. That wouldn't cost as much, but there'd be bumps along the way. So we just scaled everything up recently to buy us time to go work on figuring out how to improve some of these auto scaling. Interesting. Yeah. You could spend two weeks trying to figure out the right way to go to production, or you could spend just more money because you might not be sure with the multiple month life cycle, some of these things. Right. Is this actually going to be the way we want to stick with so let's not spend two weeks optimizing it first. Right. Very interesting. And I mean, look, not every company can make that decision. Like, if you are bootstrapped or you're trying to get off the ground, which a lot of companies are, you do have to make those. You can't just pay your way to the future. Yeah. And I'm a big fan of bootstrapped companies and finding your way, I don't think that necessarily just set a ton of money on fire. Right. The only way forward. But if you have backers already, then they would prefer you to move faster, I suspect. Correct? Yeah, correct. I always was self conscious about our operating costs as an ML company because they're high compared to other SaaS companies where you don't have heavy compute. But the investors we work with, they get they're like, okay, there's nothing that fundamental about this that requires those costs to be high. You just have to spend time on bringing them down. And there's like a clear path. It's not like Uber, where it's like the path to bring costs down, or like self driving cars because it's expensive to employ humans that's like, so far down the road. But for us, it's like, okay, we need to just spend three months making these models more efficient, and they'll run a lot cheaper, but it's that trade off. But I love bootstrap companies, too. I mean, just a different way to do it. Something special about like, you're actually making a profit and you're actually view of customers and knowing yeah. And the freedom for sure. Yeah. So you probably saw me mess around with the screen here to pull up this Raspberry pi thing. There's a question out in the audience says, could you do this kind of stuff on a raspberry pi? And like a standard raspberry pi, I suspect absolutely no. Have you ever seen that there are water cooled raspberry pi clusters? Whoa. Where are these things? That is crazy. Is that insane? That's insane. What kind of compute are they getting on that? It's pretty comparable to a MacBook Pro on this. That's crazy. What? Eight water cooled raspberry PiS in a cluster? And it's really an amazing device, but if you look back at you sort of consider it like a single PC with a basic Nvidia card or a MacBook Pro or something like that. That's still pretty far from what you guys need. How many GPUs did you say you were using to train your models? Yeah, there's 64 for the bigger ones. Yeah. In parallel. Yeah. These are not small GPUs that would work. I'm going to maybe throw it out there for you and say probably no. Maybe for the Scikit learn type stuff, but not for what you're doing. Not the TensorFlow PyTorch. Yeah, not for training. But you could do inference on a raspberry pi. Like, you could squeeze a model down super tiny, like what they do to get some models onto your phones and run that on a raspberry pi. You get the model small enough, the accuracy might not be great, but you could do it. There's a lot of stuff happening around the Edge. I think a lot of that, siri. The Edge compute, the sort of ML on device type stuff, like, a lot of the speech recognition on your phone now happens on device and not in the cloud. Sort of related to this, like the new M One chips. And even the chips in the Apple phones before then come with neural engines built in, like multicore neural engines. Interesting for Edge stuff again, but not really going to let you do the training and stuff like that. Right. I haven't done much iOS development, but I know there's like, SDKs now to kind of get your neural networks on device and make use of the hardware on the phone. And definitely if you're trying to deploy your stuff on the Edge, there's a lot more resources available to you. It's a really good experience because having you speak to your assistant or you do something and it says thinking like, okay, well, I don't want that. I'll just go do it if I got to wait 10 seconds. Right. But it happens immediately. And there's the privacy aspect, too, of yeah, absolutely. The privacy is great. Yeah. Like, the wakeword on the Alexa, I don't know if you notice, but the wakewords, like, on the Alexa device, they happen local that runs locally. Although I've heard that when you say Alexa, they verify it in the cloud with a more powerful model. Interesting, because sometimes it'll trigger and then shut off. I don't know if you've ever seen that happen. Yeah, it'll spin around and go, no, that wasn't right. Yeah, exactly. I think what's happening is that they're sending the wakeword to the cloud to verify, like, did you actually say Alexa? Probably the local models below some certain confidence level. It sends it up to the cloud, and then the cloud verifies like, yes, start processing. But it is much faster from a latency perspective. Although with five G, I don't know if mobile Internet is so much faster now. It's getting pretty crazy. Yeah, absolutely. Yeah. Sometimes I'll be somewhere my WiFi is slow, and I'll just Tether my phone, and it's like, faster if I'm not at my house. I usually do that. If I go to a coffee shop or an airport, I'm like, there's a very low chance that the WiFi here is better than my 5G Tethered. Exactly. Chuck Woody out in the audience has a real interesting question, I think, that you can speak to because you're in this space right now, living it. What do investors look at when considering an AI startup? Or maybe AI startup, not just specifically speech to text? Yeah, it's a good question. I think it really depends on are you building a vertical application that makes use of AI? So you're building some call center optimization software where there's, like, AI under the hood, but you're using it to power this business use case versus are you building some infrastructure? AI company? We're building APIs for speech to text. Or if you're building a company that's exposing APIs for NLP or different types of tasks, I think it varies what they look at. I am not an expert in fundraising or AI startups. I want to make that very clear. So maybe don't take my advice too seriously. Yeah, but you've done it successfully, which is yeah, I mean, there are people who claim to be experts but are not currently running a successful backed company. Sure. I wouldn't put too much of a caveat there. Yeah, I think we just got lucky with meeting some of the right people that have helped us. But I think it's like, yeah. Are you doing something innovative on the model side? Do you have some innovation on the architecture side? I actually don't really think the whole data vote is that strong of an argument, personally, because there's just so much data on the Internet now, and data moat being like, we run Gmail so we can scan everybody's email. That gives us a competitive advantage. Something like that. Exactly. I don't know. You might get, like, a. Slight advantage. But there's so much data on the internet and there's so many innovations happening around. Like, look at GPT-3 that OpenAI put out, right. That was just trained on, like, crazy amount, a huge model trained on crazy amounts of public domain data on the Internet works so well across so many different tasks. So even if you had a data mode for a specific task, it's arguable that GPT-3 could beat you at that task. So I think it depends what you're doing. But I don't personally buy into the whole data mode thing that much, because even for us, we're able to build some of the best speech to text models in the world. And we don't have this secret source of data. We just have a lot of innovation on the model side, and there's tons of data in the public domain that you can access now. So I think it's really about, like, are you building some type of application that is making the lives of a customer developer, some startup, easier leveraging AI? Are you solving a problem we'll pay money to solve? Yeah, exactly, because I actually think it's more about the distribution of the tech you're building versus the tech itself. So are you packaging it up in an easy to use API? Or imagine you're selling something to podcast hosts that uses AI. I mean, AI could be amazing, but if the user interface sucks, you're not going to use what you do. You're going to make a post request over to this and you put this header in and here's how you do paging. No, here's the library. In your language, you call the one function, things happen. Right. Like, how presentable or straightforward do you make it? Right, right, because I actually think that's a huge piece of it. Are you making it easier? Is the distribution around the technology you're creating really powerful? And do you have good ideas around that? So I think it's a combination of those things, but to be honest, I think really depends on what you're building and what the product is or what you're doing, because it varies. It varies a lot. Yeah. There's also the part that we as developers don't love to think about, but the marketing and awareness and growth and traction. You could say, look, here's the most amazing model we have. Well, we haven't actually got any users yet, but that is a really hard sell for investors unless they absolutely see this has huge potential. Right, but if you're like, look, we've got this much monthly number of users and here's the way we're going to start to create a premium offering. That's something we're not particularly skilled at as developers, but that's a non trivial part of any tech startup, right? Oh, yeah. And I think as a developer, too, you kind of like shy away from wanting to work on that because it's so much easier to just write code or build a new feature versus, like, go solve this hard marketing problem, or go marketing sales. You got to have them. Even if you're bad at it, you don't like it. Yeah, we're fortunate that we get to market to developers. So I enjoy it because you get to talk to developers all the time. But, yeah, that's a huge piece of it, too. Definitely. It's got to all come together. Yeah. This up a little bit. We're getting sort of near the end, but let's talk about you've got this idea of you've got your models, you've got your libraries, you've trained them up using your GPUs. Now you want to offer it as an API. How do you go to production with a machine learning model and do something interesting? You want to talk about how that's worked. I know you talked a little bit about running the cloud and whatnot, but yeah. Do you offer as an API over Flask or run it in a cloud? Like, what are you doing there? Are they lambda functions? Yeah. It's a good question. What's your world look like? So we have Asynchronous APIs where you send in an audio file, and then we send you a webhook when it's done processing. And then we have real time APIs over WebSocket, where you're streaming audio and you're getting stuff back over a WebSocket in real time. The real time stuff is a lot more challenging to build. But, yeah, the Async stuff really what happens is we have, like so one of our main APIs was built in tornado. I don't know if you legacy early Async enabled Python Web framework. Before async I o was officially a thing. Yeah. So I built the first version of the API in Tornado. So it's kind of, like, still in tornado for that reason. A lot of the newer things are newer microservices are built fast API or Flask. And so for the Asynchronous API, what happens is, like, you're making a post request. The API is really just like a Crud app. It's storing a record of the request that you made with all the parameters that you turned on or turn off. And then that goes into a database. Some worker that's, like, the orchestrator is constantly looking at that database, and it's like, okay, there's some new work to be done. And then kicks off all these different jobs to all these different microservices, some over queues, some over Http collects everything back orchestrates. Like, what could be done in parallel? What depends on what to be done first. When that's all done, all the kind of Asynchronous, like, background jobs, the orchestrator pushes the final result back into our primary database, and then that triggers you getting a webhook with the final result. So that's like, in a nutshell, kind of what the architecture looks like. For the Asynchronous workloads, there's, like, tons of different microservices, all with different instance types and different compute requirements. Some GPU, some CPU, all different scaling policies. And that's really where the hard part is. That's kind of like the basic overview of how the Asynchronous stuff works in production. Yeah, very cool. Yeah. Are you seeing postgres or MySQL or something like that? Postgres for the primary DB, because we're on AWS, we use DynamoDB for a couple of things, like ephemeral records we need to keep around for when you send something in, it goes to DynamoDB, and that's where we keep track of basically your request and what parameters you had on and off. That kicks off a bunch of things. But the primary DB is postgres. Yeah, I think there's like at this point, it's getting pretty large. There's like a few billion records in there because we process, like, a couple million audio files a day with the API. Wow. Sometimes I'll read on Hacker News, I think GitHub went down at one point because they couldn't increment the primary key values any higher. Int 64 is overflowing. We're done. Yeah, something like that. I'm in the back of my mind, like, I hope we're thinking about something like that, because that would be really bad if we came up against something like that. Do you store the audio content in the database, or do they go in like, some kind of bucket, some object storage thing? So we're unique in that we don't store a copy of your audio data for privacy reasons for you. So you send something in, it's stored ephemerally, like in the memory of the machine that's processing your file. And then what's stored is the transcription text encrypted at rest because you need to be able to make a get request for the API to fetch it. But then you can follow up with a delete request to permanently delete the transcription text from our database as well. So we try to keep no record of the data that you're processing because we want to be really privacy focused and sensitive. Some customers will toggle on that. We keep some of their data to continuously improve the models, but by default, we don't store anything. Yeah, that's really cool. Yeah, that's good for privacy. It's also good for you all because there's just less stuff that you have to be nervous about when you're trying to fall asleep. You're like, what if somebody broke in and got all the audio? Oh, wait, we don't have the audio. Okay, so that's not a thing. They could get things like that, right? Yeah, definitely. I hadn't thought about that before, but I'm imagining that what that would be like. Well, now I'm making now you could be nervous because there's probably other stuff for that. Yeah. Now you got me thinking about space, like, what are those things we need to lock up now we're mostly a team of engineers, so I think of the 30 people, like 70% are engineers with a lot more experience than me. So we're doing all everything, like, by the book, especially with the business that we're in. Yeah, of course. Dylan. I think we're out of time if not out of topic, so let's maybe wrap this up a little bit with the final two questions and some packages and stuff. So if you're going to work on some Python code, what editor are you using these days? I'm still using Sublime. Right on. What do you use? The OG? Easy ones. I'm mostly PyCharm. If I want to just open a single file and look at it, I'll probably use Vs code for that. That's probably just want to open that thing, not have all the project ideas around it. But I'm doing proper work. Probably PyCharm these days. Yeah, that makes sense. Yeah. And then notable Pipi project. Some library out there. I mean, you've already talked about it, like TensorFlow and some others, but anything out there you're like? Oh, you should definitely check this out. I would check out Hugging Face, if you haven't yet. It's pretty cool library. Yeah. Cool library. Yeah. Hugging Face seems like a really interesting idea. Yeah. I want to give a quick shout out to one as well. I don't know if you've seen this. Have you seen Pls, please, as an LS replacement? Chris May told me about this yesterday. Told me. And Brian for Python Bytes. Check this out. So it's a new LS that has, like, icons, and it's all developer focused. So, like, you got a virtual environment. It'll show that separately. If you got a Python file, it has a Python icon. The things that appear in the list are controlled somewhat by the Git ignore file and other things like that. And you can even do a more detailed listing where it'll show, like, the Git status of the various files. Isn't that crazy? That's really cool. Yeah, that's a python library. Pls. Pls. That's awesome. I'll check that one out. Yeah, people would check that out. Yeah. All right, Dylan, thank you so much for being on the show. It's really cool to get this look into running ML stuff in production and whatnot. Thanks for having me on. Final Call. Yeah, you bet. You want to give us Final Call to action people interested in sort of maybe doing an ML startup or even if they want to do things with AssemblyAI. If you want to check out our APIs for automatic speech and text, you can go to our website, assemblyai.com, get a free API token. You don't have to talk to anyone. You can start playing around. There's a lot of Python code samples that you can grab to get up and running pretty quickly. And then, yeah, if you're interested in ML startups, I think that one of the things that I always recommend is if you want to go, like, the funding route, definitely check out Y Combinator as a place to apply, because that really helped us get off the ground. They helped you out with a lot of credits around GPUs and resources and it helps a lot. That helped us a lot. Were you in the 2017? So was super helpful and I would highly recommend that there's also just a big community of other ML people that you can get access to through that. So that really helped and I would recommend people check that out. How about if I don't want to go one more funded yeah, so one more. There's also like, an online accelerator called Pioneer. I don't know if you've heard of this, but that's also a good one to check out too. If you don't want to go the accelerator route, then I would say yeah, really? It's just about getting a model working good enough to close your first customer, then just keep iterating. So don't get caught up in reaching state of the art or in the research. Just kind of think of the MVP model that you need to build to go win your first customer and they kind of keep going from there. Yeah. Awesome. All right, well, thanks for sharing all your experience and for being here. Yeah, thanks for having me on. This is fun. Yeah, you bet it was. Bye. Bye. This has been another episode of Talk Python to me. Thank you to our sponsors. Be sure to check out what they're offering. It really helps support the show. For over a dozen years, the Stack Overflow Podcast has been exploring what it means to be a developer and how the art and practice of software programming is changing the world. Join them on that adventure at docpython FM StackOverflow. Take some stress out of your life. Get notified immediately about errors and performance issues in your web or mobile applications with Sentry. Just visit Talkpython FM Century and get started for free. And be sure to use the promo code. Talkpython all one word. Want to level up your Python? We have one of the largest catalogs of Python video courses over at Talkpython. Our content ranges from true beginners to deeply advanced topics like memory and Async. And best of all, there's not a subscription in sight. Check it out for yourself at training. Talkpython FM. Be sure to subscribe to the show, open your favorite podcast app and search for Python. We should be right at the top. You can also find the itunes feed at itunes, the Google Play feed at Slash Play, and the Direct RSS feed at RSS on Talkpython FM. We're live streaming most of our recordings these days. If you want to be part of the show and have your comments featured on the air, be sure to subscribe to our YouTube channel at Talkpython FM YouTube. This is your host, Michael Kennedy. Thanks so much for listening. I really appreciate it. Now get out there and write some Python code.\n"
     ]
    }
   ],
   "source": [
    "import assemblyai as aai\n",
    "\n",
    "\n",
    "# the URL of the file we want to transcribe\n",
    "URL = \"https://talkpython.fm/episodes/download/356/tips-for-ml-ai-startups.mp3\"\n",
    "                                                                                                                                                       \t \n",
    "\n",
    "# configuration settings when you want punctuation, formatting, etc\n",
    "config = aai.TranscriptionConfig(\n",
    "    punctuate = True,\n",
    "    format_text = True\n",
    ")\n",
    "\n",
    "aai.settings.api_key = api_key\n",
    "transcriber = aai.Transcriber()\n",
    "\n",
    "# call the API, will block until the API call finishes\n",
    "transcript = transcriber.transcribe(URL, config)\n",
    "\n",
    "print(transcript.text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
